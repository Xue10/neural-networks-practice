{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "def get_stats(ids):\n",
    "    stats = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        stats[pair] = stats.get(pair, 0) + 1\n",
    "    return stats\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    i = 0\n",
    "    newids = []\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "import regex as re\n",
    "\n",
    "class RegexTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.pattern = GPT4_SPLIT_PATTERN\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size > 256\n",
    "        \n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        text_bytes = text.encode('utf-8')\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            self.merges[pair] = idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "    def _encode(self, text):\n",
    "        raw = text.encode('utf-8')\n",
    "        ids = list(raw)\n",
    "        while True:\n",
    "            stats = get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, pair, self.merges[pair])\n",
    "        return ids\n",
    "    def encode(self, text):\n",
    "        \n",
    "        splits = re.findall(self.compiled_pattern, text)\n",
    "        output = []\n",
    "        for s in splits:\n",
    "            output.extend(self._encode(s))\n",
    "        return output\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        raw = b''.join([self.vocab[idx] for idx in ids])\n",
    "        text = raw.decode('utf-8', errors='replace')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('taylorswift.txt') as f:\n",
    "    text = f.read()\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.train(text, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185561\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(101, 32): 256, (44, 32): 257, (100, 32): 258, (46, 32): 259, (114, 32): 260, (50, 48): 261, (115, 32): 262, (105, 110): 263, (111, 110): 264, (114, 105): 265, (116, 32): 266, (116, 104): 267, (101, 258): 268, (257, 261): 269, (97, 110): 270, (97, 114): 271, (101, 260): 272, (121, 32): 273, (97, 108): 274, (267, 256): 275, (118, 268): 276, (119, 105): 277, (101, 114): 278, (264, 32): 279, (277, 102): 280, (82, 101): 281, (83, 280): 282, (111, 260): 283, (99, 104): 284, (269, 49): 285, (111, 109): 286, (98, 272): 287, (32, 275): 288, (97, 121): 289, (101, 110): 290, (111, 114): 291, (274, 32): 292, (101, 109): 293, (46, 10): 294, (265, 101): 295, (263, 103): 296, (269, 50): 297, (116, 105): 298, (289, 108): 299}\n",
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b', ', 258: b'd ', 259: b'. ', 260: b'r ', 261: b'20', 262: b's ', 263: b'in', 264: b'on', 265: b'ri', 266: b't ', 267: b'th', 268: b'ed ', 269: b', 20', 270: b'an', 271: b'ar', 272: b'er ', 273: b'y ', 274: b'al', 275: b'the ', 276: b'ved ', 277: b'wi', 278: b'er', 279: b'on ', 280: b'wif', 281: b'Re', 282: b'Swif', 283: b'or ', 284: b'ch', 285: b', 201', 286: b'om', 287: b'ber ', 288: b' the ', 289: b'ay', 290: b'en', 291: b'or', 292: b'al ', 293: b'em', 294: b'.\\n', 295: b'rie', 296: b'ing', 297: b', 202', 298: b'ti', 299: b'ayl'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.merges)\n",
    "print(tokenizer.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "input = text\n",
    "ids = tokenizer.encode(input)\n",
    "\n",
    "out = tokenizer.decode(ids)\n",
    "print(input == out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "gpt4pat = re.compile(GPT4_SPLIT_PATTERN)\n",
    "input = \"Hello've world123 how's are you!!!?\"\n",
    "chunks = re.findall(gpt4pat, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
